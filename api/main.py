"""
FastAPI - API de classification de produits e-commerce (VERSION CORRIG√âE)
Chargement automatique des mod√®les avec gestion d'erreurs
"""

from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, Dict
import uvicorn
import numpy as np
from PIL import Image
import io
import joblib
import logging
from pathlib import Path

# Configuration logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Chemins des mod√®les (ADAPT√âS √Ä VOS FICHIERS)
MODELS_DIR = Path("../models")
TEXT_MODEL_PATH = MODELS_DIR / "final_best_model.pkl"
VECTORIZER_PATH = MODELS_DIR / "tfidf_vectorizer.pkl"
IMAGE_MODEL_PATH = MODELS_DIR / "cnn_final.keras"
LABEL_ENCODER_PATH = MODELS_DIR / "label_encoders.pkl" 

# Initialisation FastAPI
app = FastAPI(
    title="E-commerce Product Classification API",
    description="API de classification de produits √† partir de texte ou images",
    version="1.0.0"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================================
# CAT√âGORIES
# ============================================================================

CATEGORIES = [
    "Baby Care",
    "Beauty and Personal Care",
    "Computers",
    "Home Decor & Festive Needs",
    "Home Furnishing",
    "Kitchen & Dining",
    "Watches"
]

# ============================================================================
# GESTIONNAIRE DE MOD√àLES AM√âLIOR√â
# ============================================================================

class ModelManager:
    """Gestionnaire de chargement des mod√®les avec fallback intelligent"""
    
    def __init__(self):
        self.text_model = None
        self.text_vectorizer = None
        self.image_model = None
        self.label_encoder = None
        self.mode = "unknown"
        
    def load_all_models(self):
        """Charger tous les mod√®les disponibles"""
        logger.info("üîÑ Tentative de chargement des mod√®les...")
        
        # Charger mod√®les texte
        text_loaded = self.load_text_models()
        
        # Charger mod√®les image
        image_loaded = self.load_image_models()
        
        # D√©terminer le mode
        if text_loaded and image_loaded:
            self.mode = "full"
            logger.info("‚úÖ Mode COMPLET : Texte + Images")
        elif text_loaded:
            self.mode = "text_only"
            logger.info("‚úÖ Mode TEXTE uniquement")
        elif image_loaded:
            self.mode = "image_only"
            logger.info("‚úÖ Mode IMAGE uniquement")
        else:
            self.mode = "simulation"
            logger.warning("‚ö†Ô∏è Mode SIMULATION : Aucun mod√®le charg√©")
        
        return self.mode
    
    def load_text_models(self):
        """Charger les mod√®les texte"""
        try:
            if TEXT_MODEL_PATH.exists() and VECTORIZER_PATH.exists():
                self.text_model = joblib.load(TEXT_MODEL_PATH)
                self.text_vectorizer = joblib.load(VECTORIZER_PATH)
                logger.info("‚úÖ Mod√®les texte charg√©s")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è Fichiers mod√®les texte non trouv√©s:")
                logger.warning(f"   {TEXT_MODEL_PATH}: {TEXT_MODEL_PATH.exists()}")
                logger.warning(f"   {VECTORIZER_PATH}: {VECTORIZER_PATH.exists()}")
                return False
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement mod√®les texte: {e}")
            return False
    
    def load_image_models(self):
        """Charger les mod√®les image"""
        try:
            # Importer TensorFlow seulement si n√©cessaire
            import tensorflow as tf
            from tensorflow import keras
            
            if IMAGE_MODEL_PATH.exists() and LABEL_ENCODER_PATH.exists():
                self.image_model = keras.models.load_model(IMAGE_MODEL_PATH)
                self.label_encoder = joblib.load(LABEL_ENCODER_PATH)
                logger.info("‚úÖ Mod√®les image charg√©s")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è Fichiers mod√®les image non trouv√©s:")
                logger.warning(f"   {IMAGE_MODEL_PATH}: {IMAGE_MODEL_PATH.exists()}")
                logger.warning(f"   {LABEL_ENCODER_PATH}: {LABEL_ENCODER_PATH.exists()}")
                return False
        except ImportError:
            logger.warning("‚ö†Ô∏è TensorFlow non install√© - Mod√®les image d√©sactiv√©s")
            return False
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement mod√®les image: {e}")
            return False
    
    def predict_text(self, text: str) -> Dict:
        """Pr√©diction texte avec fallback intelligent"""
        
        # Si mod√®le disponible, utiliser ML
        if self.text_model and self.text_vectorizer:
            try:
                # Vectorisation
                X = self.text_vectorizer.transform([text])
                
                # Pr√©diction
                prediction = self.text_model.predict(X)[0]
                
                # Probabilit√©s
                if hasattr(self.text_model, 'predict_proba'):
                    probas = self.text_model.predict_proba(X)[0]
                else:
                    # Si pas de proba, cr√©er artificielle
                    probas = np.zeros(len(CATEGORIES))
                    probas[prediction] = 0.95
                    # Distribuer le reste
                    remaining = 0.05 / (len(CATEGORIES) - 1)
                    for i in range(len(CATEGORIES)):
                        if i != prediction:
                            probas[i] = remaining
                
                # R√©cup√©rer le nom de la cat√©gorie
                predicted_class = CATEGORIES[prediction]
                confidence = float(probas[prediction])
                
                # Formatter les probabilit√©s
                probabilities = {
                    cat: float(probas[i]) 
                    for i, cat in enumerate(CATEGORIES)
                }
                
                return {
                    "predicted_class": predicted_class,
                    "confidence": confidence,
                    "probabilities": probabilities,
                    "source": "ml_model"
                }
                
            except Exception as e:
                logger.error(f"Erreur pr√©diction ML texte: {e}")
                # Fallback sur simulation
                return self._simulate_text_prediction(text)
        
        # Fallback : simulation intelligente
        return self._simulate_text_prediction(text)
    
    def predict_image(self, image: Image.Image) -> Dict:
        """Pr√©diction image avec fallback"""
        
        # Si mod√®le disponible, utiliser DL
        if self.image_model and self.label_encoder:
            try:
                # Pr√©traitement
                img = image.resize((224, 224))
                img_array = np.array(img)
                
                # V√©rifier que c'est RGB
                if len(img_array.shape) == 2:  # Grayscale
                    img_array = np.stack([img_array] * 3, axis=-1)
                elif img_array.shape[2] == 4:  # RGBA
                    img_array = img_array[:, :, :3]
                
                # Normalisation
                img_array = img_array / 255.0
                img_array = np.expand_dims(img_array, axis=0)
                
                # Pr√©diction
                probas = self.image_model.predict(img_array, verbose=0)[0]
                
                # R√©sultats
                predicted_idx = np.argmax(probas)
                predicted_class = self.label_encoder.classes_[predicted_idx]
                confidence = float(probas[predicted_idx])
                
                probabilities = {
                    self.label_encoder.classes_[i]: float(probas[i])
                    for i in range(len(probas))
                }
                
                return {
                    "predicted_class": predicted_class,
                    "confidence": confidence,
                    "probabilities": probabilities,
                    "source": "dl_model"
                }
                
            except Exception as e:
                logger.error(f"Erreur pr√©diction DL image: {e}")
                return self._simulate_image_prediction()
        
        # Fallback : simulation
        return self._simulate_image_prediction()
    
    def _simulate_text_prediction(self, text: str) -> Dict:
        """Simulation intelligente bas√©e sur mots-cl√©s"""
        text_lower = text.lower()
        
        # Dictionnaire de mots-cl√©s par cat√©gorie
        keywords = {
            "Baby Care": ['baby', 'infant', 'diaper', 'newborn', 'toddler', 'b√©b√©', 'nouveau-n√©'],
            "Beauty and Personal Care": ['cosmetic', 'makeup', 'beauty', 'lipstick', 'perfume', 'skincare', 'lotion'],
            "Computers": ['computer', 'laptop', 'gaming', 'pc', 'keyboard', 'mouse', 'monitor', 'processor'],
            "Home Decor & Festive Needs": ['decoration', 'festive', 'ornament', 'vase', 'candle', 'frame'],
            "Home Furnishing": ['furniture', 'sofa', 'bed', 'chair', 'table', 'cushion', 'curtain'],
            "Kitchen & Dining": ['kitchen', 'dining', 'cookware', 'pan', 'utensil', 'plate', 'bowl'],
            "Watches": ['watch', 'timepiece', 'wristwatch', 'clock', 'chronograph']
        }
        
        # Compter les matches par cat√©gorie
        scores = {}
        for category, words in keywords.items():
            score = sum(1 for word in words if word in text_lower)
            scores[category] = score
        
        # Si au moins un match
        if max(scores.values()) > 0:
            predicted = max(scores, key=scores.get)
            conf = min(0.95, 0.60 + (scores[predicted] * 0.10))
        else:
            # Aucun match : pr√©diction al√©atoire faible confiance
            predicted = np.random.choice(CATEGORIES)
            conf = 0.30
        
        # Distribuer probabilit√©s
        probabilities = {}
        remaining = 1.0 - conf
        
        for cat in CATEGORIES:
            if cat == predicted:
                probabilities[cat] = conf
            else:
                probabilities[cat] = remaining / (len(CATEGORIES) - 1)
        
        return {
            "predicted_class": predicted,
            "confidence": float(conf),
            "probabilities": probabilities,
            "source": "simulation"
        }
    
    def _simulate_image_prediction(self) -> Dict:
        """Simulation al√©atoire pour images"""
        predicted = np.random.choice(CATEGORIES)
        conf = np.random.uniform(0.30, 0.60)
        
        probabilities = {}
        remaining = 1.0 - conf
        
        for cat in CATEGORIES:
            if cat == predicted:
                probabilities[cat] = conf
            else:
                probabilities[cat] = remaining / (len(CATEGORIES) - 1)
        
        return {
            "predicted_class": predicted,
            "confidence": float(conf),
            "probabilities": probabilities,
            "source": "simulation"
        }

# Instance globale
model_manager = ModelManager()

# ============================================================================
# MOD√àLES PYDANTIC
# ============================================================================

class TextInput(BaseModel):
    text: str

class PredictionResponse(BaseModel):
    predicted_class: str
    confidence: float
    probabilities: Dict[str, float]
    source: str = "unknown"

class HealthResponse(BaseModel):
    status: str
    mode: str
    text_model_loaded: bool
    image_model_loaded: bool

# ============================================================================
# ENDPOINTS
# ============================================================================

@app.get("/")
async def root():
    """Endpoint racine"""
    return {
        "message": "API de classification de produits e-commerce",
        "version": "1.0.0",
        "mode": model_manager.mode,
        "endpoints": {
            "health": "/health",
            "predict_text": "/predict/text",
            "predict_image": "/predict/image",
            "predict_multimodal": "/predict/multimodal",
            "categories": "/categories"
        }
    }

@app.get("/health", response_model=HealthResponse)
async def health():
    """Health check"""
    return {
        "status": "healthy",
        "mode": model_manager.mode,
        "text_model_loaded": model_manager.text_model is not None,
        "image_model_loaded": model_manager.image_model is not None
    }

@app.get("/categories")
async def get_categories():
    """Liste des cat√©gories"""
    return {
        "categories": CATEGORIES,
        "count": len(CATEGORIES)
    }

@app.post("/predict/text", response_model=PredictionResponse)
async def predict_from_text(input_data: TextInput):
    """Pr√©diction √† partir de texte"""
    try:
        result = model_manager.predict_text(input_data.text)
        return result
    except Exception as e:
        logger.error(f"Erreur pr√©diction texte: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/predict/image", response_model=PredictionResponse)
async def predict_from_image(file: UploadFile = File(...)):
    """Pr√©diction √† partir d'image"""
    try:
        # V√©rifier type
        if not file.content_type.startswith('image/'):
            raise HTTPException(status_code=400, detail="Le fichier doit √™tre une image")
        
        # Charger image
        contents = await file.read()
        image = Image.open(io.BytesIO(contents))
        
        # Convertir en RGB
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Pr√©diction
        result = model_manager.predict_image(image)
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur pr√©diction image: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/predict/multimodal")
async def predict_multimodal(
    text: Optional[str] = Form(None),
    file: Optional[UploadFile] = File(None)
):
    """Pr√©diction multimodale"""
    if not text and not file:
        raise HTTPException(status_code=400, detail="Au moins un input requis")
    
    try:
        predictions = []
        weights = []
        sources = []
        
        # Pr√©diction texte
        if text:
            text_pred = model_manager.predict_text(text)
            predictions.append(text_pred["probabilities"])
            weights.append(0.7)  # Poids plus √©lev√© pour texte (meilleur mod√®le)
            sources.append(text_pred["source"])
        
        # Pr√©diction image
        if file:
            contents = await file.read()
            image = Image.open(io.BytesIO(contents))
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            image_pred = model_manager.predict_image(image)
            predictions.append(image_pred["probabilities"])
            weights.append(0.3)
            sources.append(image_pred["source"])
        
        # Fusion
        if len(predictions) == 1:
            combined_probs = predictions[0]
        else:
            weights = np.array(weights) / sum(weights)
            combined_probs = {}
            for cat in CATEGORIES:
                probs = [pred.get(cat, 0.0) for pred in predictions]
                combined_probs[cat] = float(np.average(probs, weights=weights))
        
        predicted_class = max(combined_probs.items(), key=lambda x: x[1])[0]
        confidence = combined_probs[predicted_class]
        
        return {
            "predicted_class": predicted_class,
            "confidence": confidence,
            "probabilities": combined_probs,
            "source": f"multimodal({'+'.join(sources)})",
            "mode": "text+image" if len(predictions) == 2 else ("text" if text else "image")
        }
        
    except Exception as e:
        logger.error(f"Erreur pr√©diction multimodale: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# STARTUP
# ============================================================================

@app.on_event("startup")
async def startup_event():
    """Charger les mod√®les au d√©marrage"""
    logger.info("=" * 70)
    logger.info("üöÄ D√âMARRAGE DE L'API")
    logger.info("=" * 70)
    
    # Charger les mod√®les
    mode = model_manager.load_all_models()
    
    logger.info("=" * 70)
    logger.info(f"‚úÖ API PR√äTE - Mode: {mode.upper()}")
    logger.info("=" * 70)

# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True
    )