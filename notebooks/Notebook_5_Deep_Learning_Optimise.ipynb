{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ NOTEBOOK 5 : DEEP LEARNING OPTIMIS√â (GPU 4GB)\n",
    "\n",
    "## Objectifs\n",
    "- ‚úÖ Atteindre **85%+ accuracy**\n",
    "- ‚úÖ **< 30 minutes** d'entra√Ænement par mod√®le\n",
    "- ‚úÖ Optimis√© pour **GPU 4GB** + **~150 images/classe**\n",
    "- ‚úÖ Transfer Learning (EfficientNetV2-S)\n",
    "- ‚úÖ Mixed Precision Training (FP16)\n",
    "- ‚úÖ Sauvegarder mod√®le pour API\n",
    "\n",
    "## Strat√©gie\n",
    "1. Transfer Learning depuis ImageNet\n",
    "2. Progressive unfreezing (3 phases)\n",
    "3. Data augmentation intensive\n",
    "4. MixUp r√©gularisation\n",
    "5. Mixed precision (√©conomie m√©moire)\n",
    "\n",
    "**Bas√© sur la recherche 2024-2025 : EfficientNetV2 + PyTorch pour meilleur compromis vitesse/pr√©cision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "import json\n",
    "import time\n",
    "\n",
    "# V√©rifier GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üéÆ Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"Data\"\n",
    "IMAGES_DIR = DATA_DIR / \"Images\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\" / \"deep_learning\"\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "METADATA_FILE = DATA_DIR / \"data_images_corrected.csv\"\n",
    "\n",
    "# Hyperparam√®tres (optimis√©s pour GPU 4GB)\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16  # Safe pour 4GB avec FP16\n",
    "EPOCHS_PHASE1 = 3  # Classification head seulement\n",
    "EPOCHS_PHASE2 = 5  # Partial unfreeze\n",
    "EPOCHS_PHASE3 = 7  # Full fine-tune\n",
    "TOTAL_EPOCHS = EPOCHS_PHASE1 + EPOCHS_PHASE2 + EPOCHS_PHASE3\n",
    "NUM_WORKERS = 4\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "CATEGORIES = [\n",
    "    \"Baby Care\",\n",
    "    \"Beauty and Personal Care\",\n",
    "    \"Computers\",\n",
    "    \"Home Decor & Festive Needs\",\n",
    "    \"Home Furnishing\",\n",
    "    \"Kitchen & Dining\",\n",
    "    \"Watches\"\n",
    "]\n",
    "NUM_CLASSES = len(CATEGORIES)\n",
    "\n",
    "print(f\"üì¶ Config:\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Epochs: {TOTAL_EPOCHS} (3+5+7 phases)\")\n",
    "print(f\"   Classes: {NUM_CLASSES}\")\n",
    "print(f\"   Image size: {IMG_SIZE}√ó{IMG_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CHARGEMENT DONN√âES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger m√©tadonn√©es\n",
    "df = pd.read_csv(METADATA_FILE)\n",
    "print(f\"Dataset: {len(df)} images\")\n",
    "\n",
    "# V√©rifier images\n",
    "valid_idx = [i for i, row in df.iterrows() if (IMAGES_DIR / row['image']).exists()]\n",
    "df = df.loc[valid_idx].reset_index(drop=True)\n",
    "print(f\"‚úÖ {len(df)} images valides\")\n",
    "\n",
    "# Encoder labels\n",
    "label_map = {cat: i for i, cat in enumerate(CATEGORIES)}\n",
    "df['label'] = df['main_category'].map(label_map)\n",
    "\n",
    "# Split 70/15/15\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.3, random_state=RANDOM_STATE, stratify=df['label']\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, random_state=RANDOM_STATE, stratify=temp_df['label']\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Split:\")\n",
    "print(f\"   Train: {len(train_df)}\")\n",
    "print(f\"   Val:   {len(val_df)}\")\n",
    "print(f\"   Test:  {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DATA AUGMENTATION\n",
    "\n",
    "**Bas√© sur recherche 2024**: TrivialAugment pour simplicit√© + performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation ImageNet (requis pour transfer learning)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Augmentation train (intensive)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.75, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.TrivialAugmentWide(),  # Auto-augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# Val/Test (sans augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Transforms configur√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DATASET CUSTOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductDataset(Dataset):\n",
    "    \"\"\"Dataset custom pour produits\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, images_dir, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = self.images_dir / row['image']\n",
    "        \n",
    "        # Charger image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = row['label']\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Cr√©er datasets\n",
    "train_dataset = ProductDataset(train_df, IMAGES_DIR, train_transform)\n",
    "val_dataset = ProductDataset(val_df, IMAGES_DIR, val_transform)\n",
    "test_dataset = ProductDataset(test_df, IMAGES_DIR, val_transform)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataloaders cr√©√©s\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches:   {len(val_loader)}\")\n",
    "print(f\"   Test batches:  {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MOD√àLE : EfficientNetV2-S\n",
    "\n",
    "**Choix bas√© sur recherche**: Meilleur compromis vitesse/pr√©cision pour GPU 4GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes=7, pretrained=True):\n",
    "    \"\"\"EfficientNetV2-S avec classification head custom\"\"\"\n",
    "    \n",
    "    # Base pr√©-entra√Æn√©e\n",
    "    model = models.efficientnet_v2_s(weights='IMAGENET1K_V1' if pretrained else None)\n",
    "    \n",
    "    # Remplacer classifier\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.4),\n",
    "        nn.Linear(in_features, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.Dropout(p=0.3),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Cr√©er mod√®le\n",
    "model = build_model(num_classes=NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"‚úÖ Mod√®le cr√©√©\")\n",
    "print(f\"   Total params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MIXUP R√âGULARISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"MixUp: m√©langer images et labels\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Loss pour MixUp\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "print(\"‚úÖ MixUp configur√© (alpha=0.2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. FONCTIONS D'ENTRA√éNEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, scaler, use_mixup=True):\n",
    "    \"\"\"Entra√Æner 1 epoch avec mixed precision\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Train\")\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # MixUp\n",
    "        if use_mixup:\n",
    "            images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=0.2)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed precision\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(images)\n",
    "            \n",
    "            if use_mixup:\n",
    "                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        # Accuracy (sans MixUp pour simplicit√©)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0) if not use_mixup else labels_a.size(0)\n",
    "        if use_mixup:\n",
    "            correct += (lam * predicted.eq(labels_a).sum().item() + \n",
    "                       (1-lam) * predicted.eq(labels_b).sum().item())\n",
    "        else:\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': loss.item(), 'acc': 100.*correct/total})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def val_epoch(model, loader, criterion):\n",
    "    \"\"\"Validation epoch\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Val\")\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': loss.item(), 'acc': 100.*correct/total})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"‚úÖ Fonctions train/val pr√™tes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ENTRA√éNEMENT PROGRESSIF (3 PHASES)\n",
    "\n",
    "### PHASE 1 : Classification Head Seulement (3 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PHASE 1 : CLASSIFICATION HEAD (backbone gel√©)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Geler backbone\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.classifier.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training\n",
    "history_p1 = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "best_acc_p1 = 0.0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS_PHASE1):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS_PHASE1}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scaler, use_mixup=True)\n",
    "    val_loss, val_acc = val_epoch(model, val_loader, criterion)\n",
    "    \n",
    "    history_p1['train_loss'].append(train_loss)\n",
    "    history_p1['train_acc'].append(train_acc)\n",
    "    history_p1['val_loss'].append(val_loss)\n",
    "    history_p1['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
    "    print(f\"Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
    "    \n",
    "    if val_acc > best_acc_p1:\n",
    "        best_acc_p1 = val_acc\n",
    "        torch.save(model.state_dict(), MODELS_DIR / 'best_phase1.pth')\n",
    "        print(f\"‚úÖ Meilleur mod√®le Phase 1: {best_acc_p1:.2f}%\")\n",
    "\n",
    "phase1_time = time.time() - start_time\n",
    "print(f\"\\n‚è±Ô∏è Phase 1: {phase1_time/60:.1f} min\")\n",
    "print(f\"‚úÖ Best Val Acc Phase 1: {best_acc_p1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHASE 2 : Partial Unfreeze (5 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 2 : PARTIAL UNFREEZE (derniers blocks)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Charger meilleur mod√®le Phase 1\n",
    "model.load_state_dict(torch.load(MODELS_DIR / 'best_phase1.pth'))\n",
    "\n",
    "# D√©geler derniers 30% du backbone\n",
    "total_layers = len(list(model.features.parameters()))\n",
    "unfreeze_from = int(total_layers * 0.7)\n",
    "\n",
    "for i, param in enumerate(model.features.parameters()):\n",
    "    if i >= unfreeze_from:\n",
    "        param.requires_grad = True\n",
    "\n",
    "print(f\"D√©gel de {total_layers - unfreeze_from}/{total_layers} couches\")\n",
    "\n",
    "# Optimizer avec learning rates diff√©renci√©s\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': model.features.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_PHASE2)\n",
    "\n",
    "# Training\n",
    "history_p2 = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "best_acc_p2 = 0.0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS_PHASE2):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS_PHASE2}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scaler)\n",
    "    val_loss, val_acc = val_epoch(model, val_loader, criterion)\n",
    "    \n",
    "    history_p2['train_loss'].append(train_loss)\n",
    "    history_p2['train_acc'].append(train_acc)\n",
    "    history_p2['val_loss'].append(val_loss)\n",
    "    history_p2['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
    "    print(f\"Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
    "    \n",
    "    if val_acc > best_acc_p2:\n",
    "        best_acc_p2 = val_acc\n",
    "        torch.save(model.state_dict(), MODELS_DIR / 'best_phase2.pth')\n",
    "        print(f\"‚úÖ Meilleur mod√®le Phase 2: {best_acc_p2:.2f}%\")\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "phase2_time = time.time() - start_time\n",
    "print(f\"\\n‚è±Ô∏è Phase 2: {phase2_time/60:.1f} min\")\n",
    "print(f\"‚úÖ Best Val Acc Phase 2: {best_acc_p2:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHASE 3 : Full Fine-Tuning (7 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 3 : FULL FINE-TUNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Charger meilleur mod√®le Phase 2\n",
    "model.load_state_dict(torch.load(MODELS_DIR / 'best_phase2.pth'))\n",
    "\n",
    "# D√©geler tout\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Optimizer avec LR tr√®s faible\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_PHASE3)\n",
    "\n",
    "# Training\n",
    "history_p3 = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "best_acc_p3 = 0.0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS_PHASE3):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS_PHASE3}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scaler)\n",
    "    val_loss, val_acc = val_epoch(model, val_loader, criterion)\n",
    "    \n",
    "    history_p3['train_loss'].append(train_loss)\n",
    "    history_p3['train_acc'].append(train_acc)\n",
    "    history_p3['val_loss'].append(val_loss)\n",
    "    history_p3['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
    "    print(f\"Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
    "    \n",
    "    if val_acc > best_acc_p3:\n",
    "        best_acc_p3 = val_acc\n",
    "        torch.save(model.state_dict(), MODELS_DIR / 'best_final.pth')\n",
    "        print(f\"‚úÖ Meilleur mod√®le Final: {best_acc_p3:.2f}%\")\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "phase3_time = time.time() - start_time\n",
    "print(f\"\\n‚è±Ô∏è Phase 3: {phase3_time/60:.1f} min\")\n",
    "print(f\"‚úÖ Best Val Acc Final: {best_acc_p3:.2f}%\")\n",
    "\n",
    "total_time = phase1_time + phase2_time + phase3_time\n",
    "print(f\"\\n‚è±Ô∏è TEMPS TOTAL: {total_time/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. √âVALUATION SUR TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"√âVALUATION FINALE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Charger meilleur mod√®le\n",
    "model.load_state_dict(torch.load(MODELS_DIR / 'best_final.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Pr√©dictions\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Test\"):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = outputs.max(1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Accuracy\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"\\nüìä TEST ACCURACY: {test_acc*100:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RAPPORT PAR CAT√âGORIE\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(all_labels, all_preds, target_names=CATEGORIES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=CATEGORIES, yticklabels=CATEGORIES,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted', fontweight='bold')\n",
    "plt.ylabel('Actual', fontweight='bold')\n",
    "plt.title(f'Confusion Matrix - Test Accuracy: {test_acc*100:.2f}%', \n",
    "          fontweight='bold', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Courbes d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Combiner historiques\n",
    "all_train_loss = history_p1['train_loss'] + history_p2['train_loss'] + history_p3['train_loss']\n",
    "all_val_loss = history_p1['val_loss'] + history_p2['val_loss'] + history_p3['val_loss']\n",
    "all_train_acc = history_p1['train_acc'] + history_p2['train_acc'] + history_p3['train_acc']\n",
    "all_val_acc = history_p1['val_acc'] + history_p2['val_acc'] + history_p3['val_acc']\n",
    "\n",
    "epochs = range(1, len(all_train_loss) + 1)\n",
    "\n",
    "# Loss\n",
    "ax1.plot(epochs, all_train_loss, 'b-', label='Train', linewidth=2)\n",
    "ax1.plot(epochs, all_val_loss, 'r-', label='Val', linewidth=2)\n",
    "ax1.axvline(x=EPOCHS_PHASE1, color='gray', linestyle='--', alpha=0.5, label='Phase 1‚Üí2')\n",
    "ax1.axvline(x=EPOCHS_PHASE1+EPOCHS_PHASE2, color='gray', linestyle='--', alpha=0.5, label='Phase 2‚Üí3')\n",
    "ax1.set_xlabel('Epoch', fontweight='bold')\n",
    "ax1.set_ylabel('Loss', fontweight='bold')\n",
    "ax1.set_title('Loss Curves', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(epochs, all_train_acc, 'b-', label='Train', linewidth=2)\n",
    "ax2.plot(epochs, all_val_acc, 'r-', label='Val', linewidth=2)\n",
    "ax2.axvline(x=EPOCHS_PHASE1, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.axvline(x=EPOCHS_PHASE1+EPOCHS_PHASE2, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.axhline(y=85, color='green', linestyle='--', alpha=0.5, label='Objectif 85%')\n",
    "ax2.set_xlabel('Epoch', fontweight='bold')\n",
    "ax2.set_ylabel('Accuracy (%)', fontweight='bold')\n",
    "ax2.set_title('Accuracy Curves', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. SAUVEGARDER POUR API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder mod√®le final pour API\n",
    "final_model_path = MODELS_DIR / 'cnn_final.keras'\n",
    "\n",
    "# PyTorch ‚Üí ONNX (meilleur pour d√©ploiement)\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model, dummy_input,\n",
    "    MODELS_DIR / 'cnn_final.onnx',\n",
    "    input_names=['image'],\n",
    "    output_names=['predictions'],\n",
    "    dynamic_axes={'image': {0: 'batch'}, 'predictions': {0: 'batch'}},\n",
    "    opset_version=17\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Mod√®le ONNX export√©\")\n",
    "\n",
    "# Sauvegarder aussi en .pth\n",
    "torch.save(model.state_dict(), MODELS_DIR / 'cnn_final.pth')\n",
    "\n",
    "# Label encoder\n",
    "joblib.dump(label_map, MODELS_DIR / 'label_enconders.pkl')\n",
    "\n",
    "print(\"‚úÖ Label encoder sauvegard√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarder m√©triques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'model': 'EfficientNetV2-S',\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'best_val_acc_phase1': float(best_acc_p1),\n",
    "    'best_val_acc_phase2': float(best_acc_p2),\n",
    "    'best_val_acc_final': float(best_acc_p3),\n",
    "    'total_epochs': TOTAL_EPOCHS,\n",
    "    'training_time_minutes': float(total_time / 60),\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'img_size': IMG_SIZE,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'categories': CATEGORIES\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ ENTRA√éNEMENT TERMIN√â\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä R√©sultats finaux:\")\n",
    "print(f\"   Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"   Temps total: {total_time/60:.1f} minutes\")\n",
    "print(f\"\\nüìÅ Fichiers sauvegard√©s:\")\n",
    "print(f\"   - Mod√®le: {MODELS_DIR}/cnn_final.onnx\")\n",
    "print(f\"   - Mod√®le: {MODELS_DIR}/cnn_final.pth\")\n",
    "print(f\"   - Encoder: {MODELS_DIR}/label_enconders.pkl\")\n",
    "print(f\"   - M√©triques: {RESULTS_DIR}/metrics.json\")\n",
    "\n",
    "if test_acc >= 0.85:\n",
    "    print(\"\\nüéâ OBJECTIF ATTEINT : Accuracy ‚â• 85% !\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Accuracy sous objectif ({test_acc*100:.2f}% < 85%)\")\n",
    "    print(\"   Recommandations:\")\n",
    "    print(\"   - Augmenter epochs Phase 3\")\n",
    "    print(\"   - Ajouter plus d'augmentation\")\n",
    "    print(\"   - Essayer EfficientNetB0 (plus petit)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
