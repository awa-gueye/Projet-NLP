{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  NOTEBOOK 5 : DEEP LEARNING POUR LA CLASSIFICATION D'IMAGES\n",
    "\n",
    "## Objectifs\n",
    "1. **Classifier les produits** par catÃ©gorie Ã  partir de leurs images via Deep Learning\n",
    "2. **EntraÃ®ner plusieurs architectures CNN** :\n",
    "   - CNN custom from scratch\n",
    "   - **Transfer Learning** avec fine-tuning (ResNet50, EfficientNetB0)\n",
    "3. **Data augmentation** pour amÃ©liorer la gÃ©nÃ©ralisation\n",
    "4. **Ã‰valuer les performances** avec mÃ©triques complÃ¨tes\n",
    "5. **RÃ©capitulatif final** : RÃ©sultats modÃ¨le texte vs modÃ¨le images\n",
    "\n",
    "**7 catÃ©gories** : Baby Care, Beauty and Personal Care, Computers, Home Decor & Festive Needs, Home Furnishing, Kitchen & Dining, Watches\n",
    "\n",
    "**Approches** :\n",
    "- CNN custom (architecture lÃ©gÃ¨re)\n",
    "- ResNet50 (fine-tuning)\n",
    "- EfficientNetB0 (fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTS ET CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Images\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.keras.applications import ResNet50, EfficientNetB0\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, GlobalAveragePooling2D, BatchNormalization,\n",
    "    Conv2D, MaxPooling2D, Flatten, Activation\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Sklearn pour mÃ©triques\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# Config matplotlib\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"âœ… TensorFlow version : {tf.__version__}\")\n",
    "print(f\"âœ… Keras version : {keras.__version__}\")\n",
    "print(f\"âœ… GPU disponible : {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "# Config GPU (si disponible)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"âœ… GPU configurÃ© : memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âš ï¸ Erreur config GPU : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des chemins\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"Data\"\n",
    "IMAGES_DIR = DATA_DIR / \"Images\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "MODELS_DIR = RESULTS_DIR / \"models\"\n",
    "MODELS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Fichiers\n",
    "METADATA_FILE = DATA_DIR / \"data_images_corrected.csv\"\n",
    "\n",
    "print(f\"ğŸ“‚ Images : {IMAGES_DIR}\")\n",
    "print(f\"ğŸ“‚ MÃ©tadonnÃ©es : {METADATA_FILE}\")\n",
    "print(f\"ğŸ“‚ ModÃ¨les : {MODELS_DIR}\")\n",
    "\n",
    "# HyperparamÃ¨tres globaux\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"\\nâš™ï¸ HyperparamÃ¨tres :\")\n",
    "print(f\"   Image size : {IMG_SIZE}\")\n",
    "print(f\"   Batch size : {BATCH_SIZE}\")\n",
    "print(f\"   Max epochs : {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CHARGEMENT ET PRÃ‰PARATION DES DONNÃ‰ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les mÃ©tadonnÃ©es\n",
    "df = pd.read_csv(METADATA_FILE)\n",
    "\n",
    "print(f\"âœ… Dataset chargÃ© : {df.shape[0]} images\")\n",
    "print(f\"\\nğŸ“Š Distribution des catÃ©gories :\")\n",
    "print(df['main_category'].value_counts())\n",
    "\n",
    "# Encoder les labels\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['main_category'])\n",
    "n_classes = len(le.classes_)\n",
    "\n",
    "print(f\"\\nâœ… {n_classes} classes encodÃ©es :\")\n",
    "for idx, cat in enumerate(le.classes_):\n",
    "    print(f\"   {idx} â†’ {cat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VÃ©rifier la prÃ©sence des images\n",
    "print(\"ğŸ” VÃ©rification de la prÃ©sence des images...\\n\")\n",
    "\n",
    "missing_count = 0\n",
    "valid_indices = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    img_path = IMAGES_DIR / row['image']\n",
    "    if img_path.exists():\n",
    "        valid_indices.append(idx)\n",
    "    else:\n",
    "        missing_count += 1\n",
    "\n",
    "print(f\"âœ… Images trouvÃ©es : {len(valid_indices)}/{len(df)}\")\n",
    "print(f\"âš ï¸ Images manquantes : {missing_count}\")\n",
    "\n",
    "# Garder uniquement les images prÃ©sentes\n",
    "if missing_count > 0:\n",
    "    df = df.loc[valid_indices].reset_index(drop=True)\n",
    "    print(f\"\\nâœ… Dataset filtrÃ© : {len(df)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/validation/test : 70/15/15\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.3, random_state=RANDOM_STATE, stratify=df['label_encoded']\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, random_state=RANDOM_STATE, stratify=temp_df['label_encoded']\n",
    ")\n",
    "\n",
    "print(\"âœ… Split train/val/test (70/15/15) :\")\n",
    "print(f\"   Train : {len(train_df)} images ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Val   : {len(val_df)} images ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Test  : {len(test_df)} images ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# VÃ©rifier la distribution\n",
    "print(f\"\\nğŸ“Š Distribution dans le test set :\")\n",
    "for cat in le.classes_:\n",
    "    count = (test_df['main_category'] == cat).sum()\n",
    "    print(f\"   {cat:<35} : {count:3d} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DATA GENERATORS AVEC AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation pour le training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation et test : seulement rescaling\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "print(\"âœ… ImageDataGenerators crÃ©Ã©s\")\n",
    "print(\"\\nğŸ”„ Augmentation (train) :\")\n",
    "print(\"   - Rotation : Â±20Â°\")\n",
    "print(\"   - Translation : Â±20%\")\n",
    "print(\"   - Flip horizontal\")\n",
    "print(\"   - Zoom : Â±20%\")\n",
    "print(\"   - Shear : Â±15%\")\n",
    "print(\"\\nğŸ“ Val/Test : rescaling uniquement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour crÃ©er un gÃ©nÃ©rateur depuis un DataFrame\n",
    "def create_generator(datagen, dataframe, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    \"\"\"\n",
    "    CrÃ©e un gÃ©nÃ©rateur custom qui charge les images depuis les chemins du DataFrame\n",
    "    \"\"\"\n",
    "    def generator():\n",
    "        indices = np.arange(len(dataframe))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        \n",
    "        for start_idx in range(0, len(dataframe), batch_size):\n",
    "            batch_indices = indices[start_idx:start_idx + batch_size]\n",
    "            batch_df = dataframe.iloc[batch_indices]\n",
    "            \n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            \n",
    "            for _, row in batch_df.iterrows():\n",
    "                img_path = IMAGES_DIR / row['image']\n",
    "                try:\n",
    "                    img = keras.preprocessing.image.load_img(img_path, target_size=IMG_SIZE)\n",
    "                    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "                    batch_images.append(img_array)\n",
    "                    batch_labels.append(row['label_encoded'])\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            if batch_images:\n",
    "                X = np.array(batch_images)\n",
    "                y = to_categorical(batch_labels, num_classes=n_classes)\n",
    "                \n",
    "                # Appliquer l'augmentation\n",
    "                for i in range(len(X)):\n",
    "                    X[i] = datagen.random_transform(X[i])\n",
    "                    X[i] = datagen.standardize(X[i])\n",
    "                \n",
    "                yield X, y\n",
    "    \n",
    "    return generator\n",
    "\n",
    "# CrÃ©er les gÃ©nÃ©rateurs\n",
    "train_gen = create_generator(train_datagen, train_df, shuffle=True)\n",
    "val_gen = create_generator(val_test_datagen, val_df, shuffle=False)\n",
    "test_gen = create_generator(val_test_datagen, test_df, shuffle=False)\n",
    "\n",
    "# Calculer steps\n",
    "train_steps = len(train_df) // BATCH_SIZE\n",
    "val_steps = len(val_df) // BATCH_SIZE\n",
    "test_steps = len(test_df) // BATCH_SIZE\n",
    "\n",
    "print(f\"\\nâœ… GÃ©nÃ©rateurs crÃ©Ã©s\")\n",
    "print(f\"   Train steps per epoch : {train_steps}\")\n",
    "print(f\"   Val steps per epoch   : {val_steps}\")\n",
    "print(f\"   Test steps            : {test_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MODÃˆLE 1 : CNN CUSTOM (FROM SCRATCH)\n",
    "\n",
    "### 4.1 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_custom_cnn(input_shape=(224, 224, 3), num_classes=7):\n",
    "    \"\"\"\n",
    "    CNN custom simple mais efficace\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Block 1\n",
    "        Conv2D(32, (3, 3), padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(32, (3, 3), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 2\n",
    "        Conv2D(64, (3, 3), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(64, (3, 3), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 3\n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Dense layers\n",
    "        Flatten(),\n",
    "        Dense(256),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(128),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# CrÃ©er le modÃ¨le\n",
    "cnn_model = build_custom_cnn(input_shape=(*IMG_SIZE, 3), num_classes=n_classes)\n",
    "\n",
    "print(\"âœ… CNN custom crÃ©Ã©\")\n",
    "print(f\"\\nğŸ“Š RÃ©sumÃ© de l'architecture :\")\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Compilation et entraÃ®nement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiler le modÃ¨le\n",
    "cnn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"âœ… ModÃ¨le compilÃ©\")\n",
    "print(\"   Optimizer : Adam (lr=0.001)\")\n",
    "print(\"   Loss : categorical_crossentropy\")\n",
    "print(\"   Metrics : accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "cnn_callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        str(MODELS_DIR / 'cnn_custom_best.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"âœ… Callbacks configurÃ©s :\")\n",
    "print(\"   - EarlyStopping (patience=10)\")\n",
    "print(\"   - ReduceLROnPlateau (factor=0.5, patience=5)\")\n",
    "print(\"   - ModelCheckpoint (best val_accuracy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EntraÃ®nement\n",
    "print(\"\\nğŸ”„ EntraÃ®nement CNN custom...\")\n",
    "print(f\"â³ {EPOCHS} epochs max (early stopping activÃ©)\\n\")\n",
    "\n",
    "history_cnn = cnn_model.fit(\n",
    "    tf.data.Dataset.from_generator(\n",
    "        train_gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, *IMG_SIZE, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, n_classes), dtype=tf.float32)\n",
    "        )\n",
    "    ).repeat(),\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=tf.data.Dataset.from_generator(\n",
    "        val_gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, *IMG_SIZE, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, n_classes), dtype=tf.float32)\n",
    "        )\n",
    "    ).repeat(),\n",
    "    validation_steps=val_steps,\n",
    "    callbacks=cnn_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… EntraÃ®nement CNN terminÃ©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Courbes d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des courbes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(history_cnn.history['loss'], label='Train Loss', linewidth=2)\n",
    "ax1.plot(history_cnn.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Loss', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('CNN Custom â€” Loss', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(history_cnn.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "ax2.plot(history_cnn.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Accuracy', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('CNN Custom â€” Accuracy', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Meilleures mÃ©triques\n",
    "best_epoch = np.argmax(history_cnn.history['val_accuracy'])\n",
    "best_val_acc = history_cnn.history['val_accuracy'][best_epoch]\n",
    "best_val_loss = history_cnn.history['val_loss'][best_epoch]\n",
    "\n",
    "print(f\"\\nğŸ† Meilleures performances (epoch {best_epoch + 1}) :\")\n",
    "print(f\"   Val Accuracy : {best_val_acc:.4f}\")\n",
    "print(f\"   Val Loss     : {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Ã‰valuation sur le test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã‰valuation\n",
    "print(\"ğŸ”„ Ã‰valuation CNN custom sur le test set...\\n\")\n",
    "\n",
    "test_loss_cnn, test_acc_cnn = cnn_model.evaluate(\n",
    "    tf.data.Dataset.from_generator(\n",
    "        test_gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, *IMG_SIZE, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, n_classes), dtype=tf.float32)\n",
    "        )\n",
    "    ),\n",
    "    steps=test_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… CNN Custom â€” Test Set :\")\n",
    "print(f\"   Loss     : {test_loss_cnn:.4f}\")\n",
    "print(f\"   Accuracy : {test_acc_cnn:.4f} ({test_acc_cnn*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MODÃˆLE 2 : TRANSFER LEARNING â€” RESNET50\n",
    "\n",
    "### 5.1 Architecture avec fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet50_model(input_shape=(224, 224, 3), num_classes=7, trainable_layers=30):\n",
    "    \"\"\"\n",
    "    ResNet50 prÃ©-entraÃ®nÃ© avec fine-tuning\n",
    "    \"\"\"\n",
    "    # Charger ResNet50 prÃ©-entraÃ®nÃ© (ImageNet)\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Geler toutes les couches sauf les N derniÃ¨res\n",
    "    for layer in base_model.layers[:-trainable_layers]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Ajouter les couches de classification\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# CrÃ©er le modÃ¨le\n",
    "resnet_model, resnet_base = build_resnet50_model(\n",
    "    input_shape=(*IMG_SIZE, 3),\n",
    "    num_classes=n_classes,\n",
    "    trainable_layers=30\n",
    ")\n",
    "\n",
    "# Compter les couches gelÃ©es/entraÃ®nables\n",
    "trainable_count = sum([1 for layer in resnet_model.layers if layer.trainable])\n",
    "total_count = len(resnet_model.layers)\n",
    "\n",
    "print(\"âœ… ResNet50 crÃ©Ã©\")\n",
    "print(f\"   Couches totales : {total_count}\")\n",
    "print(f\"   Couches entraÃ®nables : {trainable_count}\")\n",
    "print(f\"   Couches gelÃ©es : {total_count - trainable_count}\")\n",
    "print(f\"\\nğŸ“Š RÃ©sumÃ© :\")\n",
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Compilation et entraÃ®nement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiler\n",
    "resnet_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),  # LR plus faible pour fine-tuning\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"âœ… ResNet50 compilÃ©\")\n",
    "print(\"   Optimizer : Adam (lr=1e-4 pour fine-tuning)\")\n",
    "\n",
    "# Callbacks\n",
    "resnet_callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        str(MODELS_DIR / 'resnet50_best.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"âœ… Callbacks configurÃ©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EntraÃ®nement\n",
    "print(\"\\nğŸ”„ EntraÃ®nement ResNet50 (fine-tuning)...\")\n",
    "print(f\"â³ {EPOCHS} epochs max\\n\")\n",
    "\n",
    "history_resnet = resnet_model.fit(\n",
    "    tf.data.Dataset.from_generator(\n",
    "        train_gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, *IMG_SIZE, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, n_classes), dtype=tf.float32)\n",
    "        )\n",
    "    ).repeat(),\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=tf.data.Dataset.from_generator(\n",
    "        val_gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, *IMG_SIZE, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, n_classes), dtype=tf.float32)\n",
    "        )\n",
    "    ).repeat(),\n",
    "    validation_steps=val_steps,\n",
    "    callbacks=resnet_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… EntraÃ®nement ResNet50 terminÃ©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Courbes d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(history_resnet.history['loss'], label='Train Loss', linewidth=2)\n",
    "ax1.plot(history_resnet.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Loss', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('ResNet50 â€” Loss', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(history_resnet.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "ax2.plot(history_resnet.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Accuracy', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('ResNet50 â€” Accuracy', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Meilleures mÃ©triques\n",
    "best_epoch_resnet = np.argmax(history_resnet.history['val_accuracy'])\n",
    "best_val_acc_resnet = history_resnet.history['val_accuracy'][best_epoch_resnet]\n",
    "best_val_loss_resnet = history_resnet.history['val_loss'][best_epoch_resnet]\n",
    "\n",
    "print(f\"\\nğŸ† Meilleures performances (epoch {best_epoch_resnet + 1}) :\")\n",
    "print(f\"   Val Accuracy : {best_val_acc_resnet:.4f}\")\n",
    "print(f\"   Val Loss     : {best_val_loss_resnet:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Ã‰valuation sur le test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã‰valuation\n",
    "print(\"ğŸ”„ Ã‰valuation ResNet50 sur le test set...\\n\")\n",
    "\n",
    "test_loss_resnet, test_acc_resnet = resnet_model.evaluate(\n",
    "    tf.data.Dataset.from_generator(\n",
    "        test_gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, *IMG_SIZE, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, n_classes), dtype=tf.float32)\n",
    "        )\n",
    "    ),\n",
    "    steps=test_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… ResNet50 â€” Test Set :\")\n",
    "print(f\"   Loss     : {test_loss_resnet:.4f}\")\n",
    "print(f\"   Accuracy : {test_acc_resnet:.4f} ({test_acc_resnet*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MODÃˆLE 3 : TRANSFER LEARNING â€” EFFICIENTNETB0\n",
    "\n",
    "### 6.1 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_efficientnet_model(input_shape=(224, 224, 3), num_classes=7, trainable_layers=50):\n",
    "    \"\"\"\n",
    "    EfficientNetB0 prÃ©-entraÃ®nÃ© avec fine-tuning\n",
    "    \"\"\"\n",
    "    base_model = EfficientNetB0(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Geler les couches sauf les N derniÃ¨res\n",
    "    for layer in base_model.layers[:-trainable_layers]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Couches de classification\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# CrÃ©er le modÃ¨le\n",
    "efficientnet_model, efficientnet_base = build_efficientnet_model(\n",
    "    input_shape=(*IMG_SIZE, 3),\n",
    "    num_classes=n_classes,\n",
    "    trainable_layers=50\n",
    ")\n",
    "\n",
    "trainable_eff = sum([1 for layer in efficientnet_model.layers if layer.trainable])\n",
    "total_eff = len(efficientnet_model.layers)\n",
    "\n",
    "print(\"âœ… EfficientNetB0 crÃ©Ã©\")\n",
    "print(f\"   Couches totales : {total_eff}\")\n",
    "print(f\"   Couches entraÃ®nables : {trainable_eff}\")\n",
    "print(f\"   Couches gelÃ©es : {total_eff - trainable_eff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Compilation et entraÃ®nement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiler\n",
    "efficientnet_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"âœ… EfficientNetB0 compilÃ©\")\n",
    "\n",
    "# Callbacks\n",
    "efficientnet_callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        str(MODELS_DIR / 'efficientnet_best.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"âœ… Callbacks configurÃ©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EntraÃ®nement\n",
    "print(\"\\nğŸ”„ EntraÃ®nement EfficientNetB0...\")\n",
    "print(f\"â³ {EPOCHS} epochs max\\n\")\n",
    "\n",
    "history_efficientnet = efficientnet_model.fit(\n",
    "    tf.data.Dataset.from_generator(\n",
    "        train_gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, *IMG_SIZE, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, n_classes), dtype=tf.float32)\n",
    "        )\n",
    "    ).repeat(),\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=tf.data.Dataset.from_generator(\n",
    "        val_gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, *IMG_SIZE, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, n_classes), dtype=tf.float32)\n",
    "        )\n",
    "    ).repeat(),\n",
    "    validation_steps=val_steps,\n",
    "    callbacks=efficientnet_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… EntraÃ®nement EfficientNetB0 terminÃ©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Courbes et Ã©valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(history_efficientnet.history['loss'], label='Train Loss', linewidth=2)\n",
    "ax1.plot(history_efficientnet.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Loss', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('EfficientNetB0 â€” Loss', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.plot(history_efficientnet.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "ax2.plot(history_efficientnet.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Accuracy', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('EfficientNetB0 â€” Accuracy', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Ã‰valuation test\n",
    "print(\"\\nğŸ”„ Ã‰valuation EfficientNetB0 sur le test set...\\n\")\n",
    "\n",
    "test_loss_eff, test_acc_eff = efficientnet_model.evaluate(\n",
    "    tf.data.Dataset.from_generator(\n",
    "        test_gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, *IMG_SIZE, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, n_classes), dtype=tf.float32)\n",
    "        )\n",
    "    ),\n",
    "    steps=test_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… EfficientNetB0 â€” Test Set :\")\n",
    "print(f\"   Loss     : {test_loss_eff:.4f}\")\n",
    "print(f\"   Accuracy : {test_acc_eff:.4f} ({test_acc_eff*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. COMPARAISON DES 3 MODÃˆLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau comparatif\n",
    "results_dl = pd.DataFrame({\n",
    "    'ModÃ¨le': ['CNN Custom', 'ResNet50', 'EfficientNetB0'],\n",
    "    'Test Loss': [test_loss_cnn, test_loss_resnet, test_loss_eff],\n",
    "    'Test Accuracy': [test_acc_cnn, test_acc_resnet, test_acc_eff]\n",
    "})\n",
    "\n",
    "results_dl = results_dl.sort_values('Test Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š COMPARAISON DES MODÃˆLES DEEP LEARNING\")\n",
    "print(\"=\"*70)\n",
    "print(results_dl.to_string(index=False))\n",
    "\n",
    "# Meilleur modÃ¨le\n",
    "best_model_name = results_dl.loc[0, 'ModÃ¨le']\n",
    "best_model_acc = results_dl.loc[0, 'Test Accuracy']\n",
    "\n",
    "print(f\"\\nğŸ† MEILLEUR MODÃˆLE : {best_model_name}\")\n",
    "print(f\"   Test Accuracy : {best_model_acc:.4f} ({best_model_acc*100:.2f}%)\")\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars = ax.barh(results_dl['ModÃ¨le'], results_dl['Test Accuracy'], \n",
    "               color=['gold', 'steelblue', 'coral'][:len(results_dl)],\n",
    "               edgecolor='black', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Comparaison des modÃ¨les Deep Learning', fontsize=13, fontweight='bold')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Valeurs sur les barres\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax.text(width + 0.01, bar.get_y() + bar.get_height()/2.,\n",
    "            f'{width:.4f}',\n",
    "            ha='left', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ANALYSE DÃ‰TAILLÃ‰E DU MEILLEUR MODÃˆLE\n",
    "\n",
    "### 8.1 PrÃ©dictions et mÃ©triques complÃ¨tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SÃ©lectionner le meilleur modÃ¨le\n",
    "if best_model_name == 'CNN Custom':\n",
    "    best_model = cnn_model\n",
    "elif best_model_name == 'ResNet50':\n",
    "    best_model = resnet_model\n",
    "else:\n",
    "    best_model = efficientnet_model\n",
    "\n",
    "# Faire des prÃ©dictions sur le test set\n",
    "print(\"ğŸ”„ GÃ©nÃ©ration des prÃ©dictions sur le test set...\\n\")\n",
    "\n",
    "y_true_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "for batch_X, batch_y in tf.data.Dataset.from_generator(\n",
    "    test_gen,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, *IMG_SIZE, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, n_classes), dtype=tf.float32)\n",
    "    )\n",
    ").take(test_steps):\n",
    "    preds = best_model.predict(batch_X, verbose=0)\n",
    "    y_true_list.extend(np.argmax(batch_y.numpy(), axis=1))\n",
    "    y_pred_list.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "y_true_arr = np.array(y_true_list)\n",
    "y_pred_arr = np.array(y_pred_list)\n",
    "\n",
    "print(f\"âœ… PrÃ©dictions gÃ©nÃ©rÃ©es : {len(y_true_arr)} Ã©chantillons\")\n",
    "\n",
    "# MÃ©triques dÃ©taillÃ©es\n",
    "test_precision = precision_score(y_true_arr, y_pred_arr, average='weighted')\n",
    "test_recall = recall_score(y_true_arr, y_pred_arr, average='weighted')\n",
    "test_f1 = f1_score(y_true_arr, y_pred_arr, average='weighted')\n",
    "\n",
    "print(f\"\\nğŸ“Š MÃ‰TRIQUES COMPLÃˆTES â€” {best_model_name}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Accuracy  : {best_model_acc:.4f} ({best_model_acc*100:.2f}%)\")\n",
    "print(f\"   Precision : {test_precision:.4f}\")\n",
    "print(f\"   Recall    : {test_recall:.4f}\")\n",
    "print(f\"   F1-Score  : {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Rapport de classification par catÃ©gorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ğŸ“Š RAPPORT DE CLASSIFICATION PAR CATÃ‰GORIE â€” {best_model_name}\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_true_arr, y_pred_arr, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_true_arr, y_pred_arr)\n",
    "\n",
    "# Brute\n",
    "plt.figure(figsize=(11, 9))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=le.classes_,\n",
    "            yticklabels=le.classes_,\n",
    "            cbar_kws={'label': 'Nombre'},\n",
    "            linewidths=0.5)\n",
    "plt.xlabel('Classe prÃ©dite', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Classe rÃ©elle', fontsize=12, fontweight='bold')\n",
    "plt.title(f'Matrice de confusion â€” {best_model_name}', \n",
    "          fontsize=13, fontweight='bold', pad=15)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# NormalisÃ©e\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(11, 9))\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Greens',\n",
    "            xticklabels=le.classes_,\n",
    "            yticklabels=le.classes_,\n",
    "            cbar_kws={'label': 'Proportion'},\n",
    "            linewidths=0.5,\n",
    "            vmin=0, vmax=1)\n",
    "plt.xlabel('Classe prÃ©dite', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Classe rÃ©elle', fontsize=12, fontweight='bold')\n",
    "plt.title(f'Matrice de confusion normalisÃ©e â€” {best_model_name}', \n",
    "          fontsize=13, fontweight='bold', pad=15)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. RÃ‰CAPITULATIF FINAL\n",
    "\n",
    "### 9.1 RÃ©sultats du modÃ¨le IMAGES (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š RÃ‰CAPITULATIF â€” CLASSIFICATION D'IMAGES PAR DEEP LEARNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ”¹ DONNÃ‰ES :\")\n",
    "print(f\"   {len(df)} images rÃ©parties en {n_classes} catÃ©gories\")\n",
    "print(f\"   Split : 70% train / 15% val / 15% test\")\n",
    "print(f\"   Images size : {IMG_SIZE}\")\n",
    "\n",
    "print(\"\\nğŸ”¹ AUGMENTATION DE DONNÃ‰ES :\")\n",
    "print(\"   Rotation, translation, flip, zoom, shear\")\n",
    "\n",
    "print(\"\\nğŸ”¹ MODÃˆLES TESTÃ‰S :\")\n",
    "print(\"   1. CNN Custom (from scratch)\")\n",
    "print(\"   2. ResNet50 (Transfer Learning + fine-tuning)\")\n",
    "print(\"   3. EfficientNetB0 (Transfer Learning + fine-tuning)\")\n",
    "\n",
    "print(f\"\\nğŸ”¹ MEILLEUR MODÃˆLE :\")\n",
    "print(f\"   {best_model_name}\")\n",
    "print(f\"   Test Accuracy : {best_model_acc:.4f} ({best_model_acc*100:.2f}%)\")\n",
    "print(f\"   Precision     : {test_precision:.4f}\")\n",
    "print(f\"   Recall        : {test_recall:.4f}\")\n",
    "print(f\"   F1-Score      : {test_f1:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ”¹ OBSERVATIONS :\")\n",
    "if best_model_acc > 0.80:\n",
    "    print(\"   âœ… Excellentes performances â€” modÃ¨le prÃªt pour la production\")\n",
    "elif best_model_acc > 0.70:\n",
    "    print(\"   âœ… Bonnes performances â€” modÃ¨le utilisable en production\")\n",
    "elif best_model_acc > 0.60:\n",
    "    print(\"   âš ï¸ Performances modÃ©rÃ©es â€” amÃ©liorations possibles\")\n",
    "else:\n",
    "    print(\"   âš ï¸ Performances faibles â€” augmentation donnÃ©es ou architectures plus complexes nÃ©cessaires\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 RÃ©sultats du modÃ¨le TEXTE (Notebook 3)\n",
    "\n",
    "**âš ï¸ IMPORTANT** : Remplacez ces valeurs par vos rÃ©sultats rÃ©els du notebook 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â¬‡ï¸ REMPLACER CES VALEURS PAR VOS RÃ‰SULTATS RÃ‰ELS DU NOTEBOOK 3\n",
    "text_results = {\n",
    "    'ModÃ¨le': 'SVM (TF-IDF)',  # â¬…ï¸ ADAPTER\n",
    "    'Accuracy': 0.9476,         # â¬…ï¸ REMPLACER\n",
    "    'Precision': 0.9450,        # â¬…ï¸ REMPLACER\n",
    "    'Recall': 0.9432,           # â¬…ï¸ REMPLACER\n",
    "    'F1-Score': 0.9432          # â¬…ï¸ REMPLACER\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š RÃ‰CAPITULATIF â€” CLASSIFICATION Ã€ PARTIR DU TEXTE (Notebook 3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ”¹ MEILLEUR MODÃˆLE TEXTE :\")\n",
    "print(f\"   {text_results['ModÃ¨le']}\")\n",
    "print(f\"   Accuracy  : {text_results['Accuracy']:.4f} ({text_results['Accuracy']*100:.2f}%)\")\n",
    "print(f\"   Precision : {text_results['Precision']:.4f}\")\n",
    "print(f\"   Recall    : {text_results['Recall']:.4f}\")\n",
    "print(f\"   F1-Score  : {text_results['F1-Score']:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Ces valeurs proviennent du Notebook 3 (features textuelles TF-IDF)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Tableau comparatif final TEXTE vs IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau rÃ©capitulatif\n",
    "recap_final = pd.DataFrame([\n",
    "    {\n",
    "        'ModalitÃ©': 'TEXTE (descriptions)',\n",
    "        'Meilleur modÃ¨le': text_results['ModÃ¨le'],\n",
    "        'Accuracy': text_results['Accuracy'],\n",
    "        'Precision': text_results['Precision'],\n",
    "        'Recall': text_results['Recall'],\n",
    "        'F1-Score': text_results['F1-Score']\n",
    "    },\n",
    "    {\n",
    "        'ModalitÃ©': 'IMAGES (Deep Learning)',\n",
    "        'Meilleur modÃ¨le': best_model_name,\n",
    "        'Accuracy': best_model_acc,\n",
    "        'Precision': test_precision,\n",
    "        'Recall': test_recall,\n",
    "        'F1-Score': test_f1\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¯ TABLEAU RÃ‰CAPITULATIF FINAL : TEXTE vs IMAGES\")\n",
    "print(\"=\"*80)\n",
    "print(recap_final.to_string(index=False))\n",
    "\n",
    "# Sauvegarder\n",
    "recap_csv = RESULTS_DIR / \"recap_final_texte_vs_images.csv\"\n",
    "recap_final.to_csv(recap_csv, index=False)\n",
    "print(f\"\\nğŸ’¾ Tableau sauvegardÃ© : {recap_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation comparative\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#4CAF50', '#2196F3']  # Vert texte, bleu images\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    values = recap_final[metric].values\n",
    "    labels = ['Texte', 'Images']\n",
    "    \n",
    "    bars = ax.bar(labels, values, color=colors, \n",
    "                  edgecolor='black', alpha=0.85, width=0.6)\n",
    "    \n",
    "    ax.set_ylabel(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(f'{metric} : Texte vs Images (Deep Learning)', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Valeurs sur barres\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom', \n",
    "                fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Comparaison finale : ModÃ¨le TEXTE vs ModÃ¨le IMAGES (Deep Learning)', \n",
    "             fontsize=15, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Conclusions finales et recommandations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“ CONCLUSIONS FINALES ET RECOMMANDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "diff_acc = text_results['Accuracy'] - best_model_acc\n",
    "diff_f1 = text_results['F1-Score'] - test_f1\n",
    "\n",
    "print(\"\\nğŸ”¹ COMPARAISON DES PERFORMANCES :\")\n",
    "print(f\"\\n   ModÃ¨le TEXTE ({text_results['ModÃ¨le']}) :\")\n",
    "print(f\"   â†’ Accuracy : {text_results['Accuracy']:.4f} ({text_results['Accuracy']*100:.2f}%)\")\n",
    "print(f\"   â†’ F1-Score : {text_results['F1-Score']:.4f}\")\n",
    "\n",
    "print(f\"\\n   ModÃ¨le IMAGES ({best_model_name}) :\")\n",
    "print(f\"   â†’ Accuracy : {best_model_acc:.4f} ({best_model_acc*100:.2f}%)\")\n",
    "print(f\"   â†’ F1-Score : {test_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n   DiffÃ©rence Accuracy : {diff_acc:+.4f} ({diff_acc*100:+.2f} points)\")\n",
    "print(f\"   DiffÃ©rence F1-Score : {diff_f1:+.4f} ({diff_f1*100:+.2f} points)\")\n",
    "\n",
    "print(\"\\nğŸ”¹ RECOMMANDATIONS POUR LA PRODUCTION :\")\n",
    "\n",
    "if diff_f1 > 0.15:\n",
    "    print(\"\\n   ğŸ¯ STRATÃ‰GIE : PrivilÃ©gier le modÃ¨le TEXTE\")\n",
    "    print(\"   1. âœ… Utiliser le modÃ¨le TEXTE comme modÃ¨le principal\")\n",
    "    print(\"   2. ğŸ’¡ ModÃ¨le IMAGES comme fallback si description manquante/courte\")\n",
    "    print(\"   3. ğŸš€ Envisager fusion multimodale pour maximiser la performance\")\n",
    "elif diff_f1 > 0.05:\n",
    "    print(\"\\n   ğŸ¯ STRATÃ‰GIE : Combiner les deux modalitÃ©s\")\n",
    "    print(\"   1. ğŸš€ ImplÃ©menter une Late Fusion (moyenne pondÃ©rÃ©e des prÃ©dictions)\")\n",
    "    print(\"   2. ğŸ’¡ Poids suggÃ©rÃ©s : 70% texte, 30% images (Ã  optimiser)\")\n",
    "    print(\"   3. âœ… Utiliser texte seul si description riche, images seules sinon\")\n",
    "elif diff_f1 > -0.05:\n",
    "    print(\"\\n   ğŸ¯ STRATÃ‰GIE : Performances similaires\")\n",
    "    print(\"   1. ğŸ”„ Choisir selon le contexte (texte riche vs images de qualitÃ©)\")\n",
    "    print(\"   2. ğŸš€ Fusion multimodale recommandÃ©e pour maximiser\")\n",
    "    print(\"   3. âš¡ ModÃ¨le IMAGES plus rapide en infÃ©rence que texte+vectorisation\")\n",
    "else:\n",
    "    print(\"\\n   ğŸ¯ STRATÃ‰GIE : PrivilÃ©gier le modÃ¨le IMAGES\")\n",
    "    print(\"   1. âœ… Utiliser le modÃ¨le IMAGES (Deep Learning) comme principal\")\n",
    "    print(\"   2. ğŸ’¡ ModÃ¨le TEXTE comme fallback ou pour validation\")\n",
    "    print(\"   3. ğŸš€ Fusion multimodale pourrait encore amÃ©liorer\")\n",
    "\n",
    "print(\"\\nğŸ”¹ AMÃ‰LIORATIONS POSSIBLES :\")\n",
    "print(\"\\n   ğŸ“¸ Pour les IMAGES (Deep Learning) :\")\n",
    "print(\"      - Plus d'Ã©poques d'entraÃ®nement (50 â†’ 100)\")\n",
    "print(\"      - Data augmentation plus agressive\")\n",
    "print(\"      - Tester d'autres architectures (Vision Transformer, ConvNeXt, Swin)\")\n",
    "print(\"      - Ensembling (moyenne de plusieurs modÃ¨les)\")\n",
    "print(\"      - Pseudo-labeling sur donnÃ©es non Ã©tiquetÃ©es\")\n",
    "\n",
    "print(\"\\n   ğŸ“ Pour le TEXTE :\")\n",
    "print(\"      - Embeddings prÃ©-entraÃ®nÃ©s (BERT, USE, Sentence-BERT)\")\n",
    "print(\"      - ModÃ¨les de langue (LSTM, Transformer)\")\n",
    "print(\"      - Augmentation de donnÃ©es textuelles (back-translation)\")\n",
    "\n",
    "print(\"\\n   ğŸ”— Pour la FUSION MULTIMODALE :\")\n",
    "print(\"      - Late Fusion : moyenne pondÃ©rÃ©e des softmax\")\n",
    "print(\"      - Intermediate Fusion : concatÃ©ner features texte + images â†’ MLP\")\n",
    "print(\"      - Early Fusion : architecture end-to-end (CNN + LSTM)\")\n",
    "print(\"      - Attention mechanism pour pondÃ©ration dynamique\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… NOTEBOOK 5 TERMINÃ‰\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nğŸ‰ Projet de classification de produits e-commerce achevÃ© !\")\n",
    "print(f\"ğŸ“Š ModÃ¨les sauvegardÃ©s dans : {MODELS_DIR}\")\n",
    "print(f\"ğŸ“Š RÃ©sultats dans : {RESULTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
